#!/usr/bin/env python3
import requests
import time
from typing import Optional


def check_ollama_service(base_url: str = "http://localhost:11434", timeout: int = 5) -> bool:
    """
    æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ
    """
    try:
        response = requests.get(f"{base_url}/", timeout=timeout)
        return response.status_code == 200
    except Exception as e:
        print(f"âŒ OllamaæœåŠ¡è¿æ¥å¤±è´¥: {str(e)}")
        return False


def check_model_availability(model: str = "qwen3:14b", base_url: str = "http://localhost:11434") -> Optional[dict]:
    """
    æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å¯ç”¨
    """
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=10)
        if response.status_code == 200:
            models = [m["name"] for m in response.json().get("models", [])]
            return {"available": model in models, "installed_models": models}
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ£€æŸ¥å¤±è´¥: {str(e)}")
    return None


def test_model_inference(model: str = "qwen3:14b", base_url: str = "http://localhost:11434") -> bool:
    """
    æµ‹è¯•æ¨¡å‹æ¨ç†åŠŸèƒ½
    """
    test_prompt = "è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š1+1ç­‰äºå‡ ï¼Ÿ"
    try:
        response = requests.post(
            f"{base_url}/api/chat",
            json={
                "model": model,
                "messages": [{"role": "user", "content": test_prompt}],
                "stream": False
            },
            timeout=60
        )
        if response.status_code == 200:
            answer = response.json()["message"]["content"]
            print(f"âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸ\næé—®: {test_prompt}\nå›ç­”: {answer}")
            return True
        else:
            print(f"âŒ æ¨¡å‹æ¨ç†å¤±è´¥ (HTTP {response.status_code}): {response.text}")
    except Exception as e:
        print(f"âŒ æ¨ç†æµ‹è¯•å¼‚å¸¸: {str(e)}")
    return False


def full_ollama_check():
    print("\nğŸ” å¼€å§‹OllamaæœåŠ¡å¥åº·æ£€æŸ¥...")

    # æ£€æŸ¥1: æœåŠ¡æ˜¯å¦è¿è¡Œ
    if not check_ollama_service():
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆå»ºè®®:")
        print("1. è¯·ç¡®è®¤OllamaæœåŠ¡å·²å¯åŠ¨ (å‘½ä»¤è¡Œè¿è¡Œ: ollama serve)")
        print("2. æ£€æŸ¥é˜²ç«å¢™æ˜¯å¦æ”¾è¡Œ11434ç«¯å£")
        print("3. å¦‚æœæ˜¯è¿œç¨‹æœåŠ¡ï¼Œè¯·ç¡®è®¤base_urlå‚æ•°æ­£ç¡®")
        return False

    print("âœ… OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ")

    # æ£€æŸ¥2: æ¨¡å‹æ˜¯å¦å¯ç”¨
    model_status = check_model_availability()
    if not model_status:
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆå»ºè®®:")
        print("1. è¿è¡Œ: ollama pull qwen3:14b")
        print("2. è¿è¡Œ: ollama list ç¡®è®¤æ¨¡å‹å­˜åœ¨")
        return False

    if not model_status["available"]:
        print(f"âŒ æ¨¡å‹ qwen3:14b æœªå®‰è£…")
        print(f"å·²å®‰è£…æ¨¡å‹: {', '.join(model_status['installed_models']) or 'æ— '}")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆå»ºè®®:")
        print("1. è¿è¡Œ: ollama pull qwen3:14b")
        print("2. æ£€æŸ¥æ¨¡å‹åç§°æ‹¼å†™æ˜¯å¦æ­£ç¡®")
        return False

    print(f"âœ… æ¨¡å‹ qwen3:14b å·²å®‰è£…")

    # æ£€æŸ¥3: å®é™…æ¨ç†æµ‹è¯•
    if not test_model_inference():
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆå»ºè®®:")
        print("1. æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿ (è¿è¡Œ: nvidia-smi)")
        print("2. å°è¯•æ›´ç®€å•çš„æ¨¡å‹æµ‹è¯•: ollama run llama2")
        print("3. æŸ¥çœ‹Ollamaæ—¥å¿—è·å–æ›´å¤šä¿¡æ¯")
        return False

    print("\nğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ŒOllamaæœåŠ¡è¿è¡Œæ­£å¸¸ï¼")
    return True


if __name__ == "__main__":
    if full_ollama_check():
        # è·å–ç³»ç»Ÿä¿¡æ¯
        try:
            sys_info = requests.get("http://localhost:11434/api/version", timeout=5).json()
            print("\nğŸ–¥ï¸ ç³»ç»Ÿä¿¡æ¯:")
            print(f"Ollamaç‰ˆæœ¬: {sys_info.get('version')}")
            print(f"APIç‰ˆæœ¬: {sys_info.get('api_version', 'N/A')}")
        except:
            pass
    else:
        print("\nâŒ OllamaæœåŠ¡æ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·æ ¹æ®ä¸Šè¿°å»ºè®®è§£å†³é—®é¢˜")