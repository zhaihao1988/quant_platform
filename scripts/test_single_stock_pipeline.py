# --- å…³é”®ä¿®å¤ï¼šå°†æ—¥å¿—é…ç½®ç§»åˆ°æ‰€æœ‰é¡¹ç›®å¯¼å…¥ä¹‹å‰ ---
import logging
# é…ç½®å¿…é¡»åœ¨å…¶ä»–æ¨¡å—ï¼ˆå®ƒä»¬å¯èƒ½ä¼šè®°å½•æ—¥å¿—ï¼‰è¢«å¯¼å…¥ä¹‹å‰å®Œæˆ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__) # è·å–å½“å‰æ¨¡å—çš„ logger

import argparse
import os
import json
from datetime import datetime, timedelta
from sqlalchemy import and_, or_, not_
from typing import List
# ç§»é™¤akshareå¯¼å…¥
# import akshare as ak

# --- ä½¿ç”¨ç°æœ‰æ•°æ®åº“ ---
from db.database import SessionLocal
from db.models import Base, StockDisclosure, StockDisclosureChunk

# --- é¡¹ç›®æ¨¡å— ---
from data_processing.scraper import fetch_announcement_text, extract_and_clean_narrative_section, extract_qa_with_ai, extract_section_from_text, remove_tables
from scripts.to_chunks import embed_and_store_disclosure_chunks
from config.settings import settings

# --- æ—¥å¿—é…ç½® (æ—§ä½ç½®) ---
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# logger = logging.getLogger(__name__) # å·²ç§»åŠ¨åˆ°é¡¶éƒ¨

# --- æµ‹è¯•è¾“å‡ºç›®å½• ---
TEST_OUTPUT_DIR = "tests/output_test"
if not os.path.exists(TEST_OUTPUT_DIR):
    os.makedirs(TEST_OUTPUT_DIR)

try:
    from dateutil.relativedelta import relativedelta
    DATEUTIL_AVAILABLE = True
except ImportError:
    DATEUTIL_AVAILABLE = False

def get_target_disclosures_for_processing(db_session, symbol: str) -> List[StockDisclosure]:
    """
    æŸ¥æ‰¾æŒ‡å®šè‚¡ç¥¨çš„ã€ç¬¦åˆç‰¹å®šä¸šåŠ¡è§„åˆ™çš„å…¬å‘Šã€‚
    å¤ç”¨ populate_selected_raw_content.py ä¸­çš„ç­›é€‰é€»è¾‘ã€‚
    """
    logger.info(f"ä¸ºè‚¡ç¥¨ {symbol} æŸ¥è¯¢éœ€è¦å¤„ç†çš„å…¬å‘Š (ä½¿ç”¨ç­›é€‰é€»è¾‘)")
    try:
        now = datetime.now()
        one_year_ago_date = (now - timedelta(days=365)).date()

        if DATEUTIL_AVAILABLE:
            three_years_ago_date = (now - relativedelta(years=3)).date()
        else:
            three_years_ago_date = (now - timedelta(days=3 * 365 + 1)).date()  # ç²—ç•¥è®¡ç®—

        annual_semi_keywords = ['å¹´åº¦æŠ¥å‘Š', 'åŠå¹´åº¦æŠ¥å‘Š']
        # ä¿ç•™"è°ƒç ”"å…³é”®è¯ï¼Œå› ä¸ºå®ƒæ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹
        other_relevant_keywords = ['è°ƒç ”'] 

        # å®šä¹‰ä¸æƒ³è¦çš„å…¬å‘Šæ ‡é¢˜å…³é”®è¯
        exclusion_keywords = [
            'æ‘˜è¦', 'è‡ªæ„¿', 'å–æ¶ˆ', 'ç›‘ç®¡', 'æ„è§',
            'å‡½', 'ç£å¯¼', 'æç¤º', 'å®¡æ ¸'
        ]

        filter_conditions = []

        # æ¡ä»¶ç»„1ï¼šæœ€è¿‘3å¹´çš„å¹´æŠ¥/åŠå¹´æŠ¥
        for kw in annual_semi_keywords:
            filter_conditions.append(
                and_(
                    StockDisclosure.title.ilike(f'%{kw}%'),
                    StockDisclosure.ann_date >= three_years_ago_date
                )
            )

        # æ¡ä»¶ç»„2ï¼šæœ€è¿‘1å¹´çš„è°ƒç ”æ´»åŠ¨ (æ ¹æ®ç”¨æˆ·æœ€æ–°åé¦ˆï¼Œä»…åŸºäºtag)
        # --- å…³é”®ä¿®æ”¹ï¼šä»…åœ¨ tag ä¸­æœç´¢ï¼Œå¹¶ä¸¥æ ¼é™å®šåœ¨1å¹´å†… ---
        filter_conditions.append(
            and_(
                StockDisclosure.tag.ilike(f'%è°ƒç ”%'),
                StockDisclosure.ann_date >= one_year_ago_date 
            )
        )

        if not filter_conditions:
            logger.warning(f"è‚¡ç¥¨ {symbol}: æœªé…ç½®æœ‰æ•ˆçš„å…³é”®è¯ç­›é€‰æ¡ä»¶ã€‚")
            return []

        combined_keyword_filters = or_(*filter_conditions)

        # åŸºç¡€æŸ¥è¯¢
        query = db_session.query(StockDisclosure).filter(
            StockDisclosure.symbol == symbol,
            combined_keyword_filters
        )

        # å¾ªç¯æ·»åŠ æ’é™¤æ¡ä»¶
        for keyword in exclusion_keywords:
            query = query.filter(not_(StockDisclosure.title.ilike(f'%{keyword}%')))

        disclosures = query.order_by(StockDisclosure.ann_date.desc()).all()

        logger.info(f"è‚¡ç¥¨ {symbol}: æ‰¾åˆ° {len(disclosures)} æ¡ç¬¦åˆç­›é€‰æ¡ä»¶çš„å…¬å‘Šã€‚")
        return disclosures
    except Exception as e:
        logger.error(f"ä¸ºè‚¡ç¥¨ {symbol} æŸ¥è¯¢å¾…å¤„ç†å…¬å‘Šæ—¶å‡ºé”™: {e}", exc_info=True)
        return []

def process_disclosure_content(disclosure_obj: StockDisclosure) -> str:
    """
    æ ¹æ®å…¬å‘Šç±»å‹å¤„ç†å†…å®¹ï¼Œå¤ç”¨ populate_selected_raw_content.py ä¸­çš„å¤„ç†é€»è¾‘ã€‚
    """
    title = disclosure_obj.title
    tag = disclosure_obj.tag if disclosure_obj.tag else ""
    
    # # æ ¹æ®ç”¨æˆ·æµ‹è¯•è¦æ±‚ï¼Œæ³¨é‡Šæ‰æ­¤éƒ¨åˆ†ï¼Œæ€»æ˜¯é‡æ–°è·å–
    # if disclosure_obj.raw_content and disclosure_obj.raw_content.strip():
    #     logger.info("æ•°æ®åº“ä¸­å·²æœ‰ raw_contentï¼Œç›´æ¥ä½¿ç”¨ã€‚")
    #     return disclosure_obj.raw_content
    
    # æ€»æ˜¯ä»URLè·å–å¹¶å¤„ç†
    logger.info("æ ¹æ®æµ‹è¯•è¦æ±‚ï¼Œæ— è®ºæ˜¯å¦å­˜åœ¨æ—§å†…å®¹ï¼Œéƒ½å°†é‡æ–°è·å– raw_content...")
    full_text = fetch_announcement_text(
        detail_url=disclosure_obj.url,
        title=title,
        tag=tag
    )
    
    if not full_text:
        logger.error("æ— æ³•ä»å…¬å‘Šä¸­æå–ä»»ä½•æ–‡æœ¬å†…å®¹ã€‚")
        return None
    
    # æ ¹æ®ç±»å‹åº”ç”¨ä¸åŒè§£æé€»è¾‘
    content_to_store = None
    
    if 'è°ƒç ”' in tag :
        logger.info("æ£€æµ‹åˆ°è°ƒç ”æ´»åŠ¨ï¼Œä½¿ç”¨AIæå–Q&A...")
        # å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
        logger.info("ä½¿ç”¨ç¡…åŸºæµåŠ¨æ¨¡å‹ 'Qwen/Qwen3-8B' è¿›è¡Œå¤„ç†...")
        qa_list = extract_qa_with_ai(full_text, model_override="Qwen/Qwen3-8B")
        if qa_list:
            # å°†Q&Aåˆ—è¡¨æ ¼å¼åŒ–ä¸ºJSONå­—ç¬¦ä¸²è¿›è¡Œå­˜å‚¨
            content_to_store = json.dumps(qa_list, ensure_ascii=False, indent=4)
            logger.info(f"æˆåŠŸæå–åˆ° {len(qa_list)} æ¡Q&Aã€‚")
        else:
            logger.warning("AIæœªèƒ½æå–åˆ°Q&Aå†…å®¹ï¼Œå°†ä½¿ç”¨å…¨æ–‡ä½œä¸ºå¤‡ç”¨ã€‚")
            content_to_store = full_text
    
    elif 'å¹´åº¦æŠ¥å‘Š' in title or 'åŠå¹´åº¦æŠ¥å‘Š' in title:
        logger.info("æ£€æµ‹åˆ°å¹´æŠ¥/åŠå¹´æŠ¥ï¼Œæå–ã€ç®¡ç†å±‚è®¨è®ºä¸åˆ†æã€‘...")
        narrative_section = extract_section_from_text(full_text, "ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ")
        if narrative_section:
            logger.info("æˆåŠŸæå–ç« èŠ‚ï¼Œå¼€å§‹æ¸…ç†è¡¨æ ¼...")
            content_to_store = remove_tables(narrative_section)
        else:
            logger.warning("æœªèƒ½æå–åˆ°'ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ'ï¼Œå°†ä½¿ç”¨æ¸…ç†åçš„å…¨æ–‡ä½œä¸ºå¤‡ç”¨ã€‚")
            # ç›´æ¥å¯¹å…¨æ–‡è¿›è¡Œè¡¨æ ¼æ¸…ç†ï¼Œè€Œä¸æ˜¯è°ƒç”¨extract_and_clean_narrative_section
            content_to_store = remove_tables(full_text)
    
    else: # å…¶ä»–ç±»å‹
        logger.info(f"å¸¸è§„å…¬å‘Š '{title}'ï¼Œä½¿ç”¨æ¸…ç†åçš„å…¨æ–‡ã€‚")
        # å¯¹å…¨æ–‡è¿›è¡ŒåŸºæœ¬çš„è¡¨æ ¼æ¸…ç†
        content_to_store = remove_tables(full_text)
    
    return content_to_store

# --- V2: æ–°å¢ç‹¬ç«‹éªŒè¯å‡½æ•° ---
def verify_data_in_new_session(disclosure_id: int) -> bool:
    """
    å¼€å¯ä¸€ä¸ªå…¨æ–°çš„ã€ç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯æ¥éªŒè¯æ•°æ®æ˜¯å¦è¢«æ°¸ä¹…æäº¤ã€‚
    è¿™æ¨¡æ‹Ÿäº†å¤–éƒ¨å·¥å…·çš„æŸ¥è¯¢è¡Œä¸ºã€‚
    """
    logger.info("--- Verification with NEW session ---")
    new_session = None
    try:
        new_session = SessionLocal()
        logger.info(f"New session created. Querying for disclosure_id: {disclosure_id}")
        count = new_session.query(StockDisclosureChunk).filter(StockDisclosureChunk.disclosure_id == disclosure_id).count()
        
        if count > 0:
            logger.info(f"âœ…âœ…âœ… ULTIMATE SUCCESS: Found {count} chunks in a new, independent session. Data is permanently stored.")
            return True
        else:
            logger.error(f"âŒâŒâŒ ULTIMATE FAILURE: Found 0 chunks in a new, independent session. Data was NOT committed.")
            return False
            
    except Exception as e:
        logger.error(f"Error during new session verification: {e}", exc_info=True)
        return False
    finally:
        if new_session:
            new_session.close()
            logger.info("New verification session closed.")


def run_batch_processing_for_stock(stock_code: str):
    """
    é’ˆå¯¹å•ä¸ªè‚¡ç¥¨ä»æ•°æ®åº“è·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„å…¬å‘Šï¼Œè¿›è¡Œæ‰¹é‡å¤„ç†ã€å‘é‡åŒ–å’Œå­˜å‚¨ã€‚
    """
    db_session = SessionLocal()

    try:
        # --- å…³é”®ä¿®å¤ï¼šåœ¨æµ‹è¯•å¼€å§‹å‰ï¼Œå¼ºåˆ¶é‡å»ºè¡¨ç»“æ„ä»¥åŒ¹é…æœ€æ–°æ¨¡å‹ ---
        # è¿™ä¸ªæ“ä½œåœ¨æ•´ä¸ªæ‰¹æ¬¡å¼€å§‹å‰åªæ‰§è¡Œä¸€æ¬¡
        logger.info("--- [BATCH PROCESSING] Forcibly resetting 'stock_disclosure_chunk' table schema ---")
        StockDisclosureChunk.__table__.drop(db_session.bind, checkfirst=True)
        logger.info("Dropped old 'stock_disclosure_chunk' table (if it existed).")
        Base.metadata.create_all(db_session.bind)
        logger.info("Recreated table(s) based on current model definitions. Schema is now up-to-date.")
        db_session.commit()
        logger.info("--- Schema reset complete. Proceeding with batch processing. ---")

        # 1. ä»æ•°æ®åº“è·å–ç¬¦åˆæ¡ä»¶çš„å…¬å‘Šåˆ—è¡¨
        logger.info(f"--- æ­¥éª¤ 1: ä»æ•°æ®åº“è·å–è‚¡ç¥¨ä»£ç  {stock_code} æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„å…¬å‘Š... ---")
        target_disclosures = get_target_disclosures_for_processing(db_session, stock_code)
        if not target_disclosures:
            logger.error(f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°è‚¡ç¥¨ {stock_code} ç¬¦åˆæ¡ä»¶çš„ä»»ä½•å…¬å‘Šã€‚")
            return
        
        logger.info(f"æ‰¾åˆ° {len(target_disclosures)} æ¡ç¬¦åˆæ¡ä»¶çš„å…¬å‘Šï¼Œå°†å…¨éƒ¨å¤„ç†ã€‚")
        
        # --- ä¸»å¾ªç¯ï¼šéå†å¹¶å¤„ç†æ¯ä¸€æ¡å…¬å‘Š ---
        for i, disclosure in enumerate(target_disclosures, 1):
            disclosure_id = disclosure.id
            title = disclosure.title
            url = disclosure.url
            ann_date = disclosure.ann_date

            logger.info(f"\n{'='*20}>> å¼€å§‹å¤„ç†ç¬¬ {i}/{len(target_disclosures)} æ¡å…¬å‘Š <<{'='*20}")
            logger.info(f"å…¬å‘Š: '{title}' (ID: {disclosure_id}, å‘å¸ƒäº {ann_date})")

            # 2. å¤„ç†æ–‡æœ¬å†…å®¹
            logger.info("--- æ­¥éª¤ 2/4: å¤„ç†å…¬å‘Šå†…å®¹... ---")
            processed_content = process_disclosure_content(disclosure)
            if not processed_content or not processed_content.strip():
                logger.error(f"å…¬å‘ŠID {disclosure_id} å¤„ç†åå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡æ­¤å…¬å‘Šã€‚")
                continue # è·³åˆ°ä¸‹ä¸€ä¸ªå¾ªç¯

            # å°†å¤„ç†å¥½çš„å†…å®¹ä¿å­˜åˆ°æ–‡ä»¶ï¼Œæ–¹ä¾¿æ£€æŸ¥ (æ–‡ä»¶ååŒ…å«IDä»¥ç¤ºåŒºåˆ†)
            output_file = os.path.join(TEST_OUTPUT_DIR, f"{stock_code}_{disclosure_id}_processed.txt")
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(processed_content)
            logger.info(f"âœ… å¤„ç†åçš„å†…å®¹å·²ä¿å­˜è‡³: {output_file}")

            # 3. ç›´æ¥è¿›è¡Œåˆ†å—å’Œå‘é‡åŒ–å­˜å‚¨
            logger.info("--- æ­¥éª¤ 3/4: å¯¹å¤„ç†çš„å†…å®¹è¿›è¡Œåˆ†å—å’Œå‘é‡åŒ–å­˜å‚¨... ---")
            chunks_stored = embed_and_store_disclosure_chunks(
                db=db_session,
                disclosure_id=disclosure_id,
                raw_text=processed_content,
                ann_date=disclosure.ann_date
            )
            logger.info(f"âœ… å…¬å‘ŠID {disclosure_id}: å‘é‡åŒ–å’Œå­˜å‚¨æµç¨‹å®Œæˆã€‚å…±å­˜å…¥ {chunks_stored} ä¸ªæ•°æ®å—ã€‚")

            # 4. éªŒè¯ç»“æœ (åœ¨åŒä¸€ä¸ªä¼šè¯å†…)
            logger.info("--- æ­¥éª¤ 4/4: (å†…éƒ¨éªŒè¯) åœ¨å½“å‰ä¼šè¯ä¸­æ£€æŸ¥å­˜å‚¨ç»“æœ... ---")
            chunk_count = db_session.query(StockDisclosureChunk).filter(
                StockDisclosureChunk.disclosure_id == disclosure_id
            ).count()
            
            if chunk_count == chunks_stored:
                logger.info(f"âœ… å…¬å‘ŠID {disclosure_id}: å†…éƒ¨éªŒè¯æˆåŠŸï¼æ•°é‡åŒ¹é… ({chunk_count})ã€‚")
            else:
                logger.error(f"âŒ å…¬å‘ŠID {disclosure_id}: å†…éƒ¨éªŒè¯å¤±è´¥ï¼å­˜å…¥æ•°({chunks_stored})ä¸æŸ¥è¯¢æ•°({chunk_count})ä¸åŒ¹é…ã€‚")
            
            # æ¯æ¬¡å¾ªç¯åæäº¤ä¸€æ¬¡ï¼Œç¡®ä¿å•ä¸ªå…¬å‘Šçš„å¤„ç†æ˜¯åŸå­æ€§çš„
            logger.info(f"Committing changes for disclosure ID {disclosure_id}...")
            db_session.commit()
            logger.info(f"{'='*20}>> å…¬å‘ŠID {disclosure_id} å¤„ç†å®Œæ¯• <<{'='*20}\n")
        
        logger.info("ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰å…¬å‘Šå¤„ç†å®Œæ¯•! ğŸ‰ğŸ‰ğŸ‰")
        
        # æœ€ç»ˆéªŒè¯å¯ä»¥è¢«ç§»é™¤æˆ–ç®€åŒ–ï¼Œå› ä¸ºæ¯æ¬¡å¾ªç¯éƒ½å·²ç»éªŒè¯
        total_chunk_count = db_session.query(StockDisclosureChunk).count()
        logger.info(f"æ•°æ®åº“ä¸­ç°åœ¨æ€»å…±æœ‰ {total_chunk_count} ä¸ªæ•°æ®å—ã€‚")

    except Exception as e:
        logger.error(f"æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        logger.error("æ­£åœ¨å›æ»šä¸»ä¼šè¯ä¸­çš„æ‰€æœ‰æœªæäº¤æ›´æ”¹...")
        db_session.rollback()
    finally:
        logger.info("æ­£åœ¨å…³é—­ä¸»æ•°æ®åº“ä¼šè¯ã€‚")
        db_session.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¯¹å•ä¸ªè‚¡ç¥¨çš„ç¬¦åˆæ¡ä»¶å…¬å‘Šè¿›è¡Œç«¯åˆ°ç«¯å¤„ç†æµ‹è¯•ã€‚")
    parser.add_argument("--stock_code", type=str, default="000887", help="è¦æµ‹è¯•çš„è‚¡ç¥¨ä»£ç ")
    args = parser.parse_args()

    # ç¡®ä¿Embedderå¯ä»¥è¢«åˆå§‹åŒ–
    if not hasattr(settings, 'EMBEDDING_MODEL_NAME') or not settings.EMBEDDING_MODEL_NAME:
        logger.critical("EMBEDDING_MODEL_NAME æœªåœ¨ settings.py ä¸­é…ç½®ã€‚è„šæœ¬æ— æ³•æ‰§è¡Œã€‚")
        exit(1)

    # è¿è¡Œæ‰¹é‡å¤„ç†
    run_batch_processing_for_stock(args.stock_code)

    logger.info("="*20 + " æ‰¹é‡å¤„ç†æµç¨‹ç»“æŸ " + "="*20) 