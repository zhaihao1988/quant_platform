# scripts/test_pipeline.py
import sys
import os
import logging
import json
import io
from datetime import date

# --- åŠ¨æ€è·¯å¾„ä¿®å¤ï¼šç¡®ä¿èƒ½æ‰¾åˆ°é¡¹ç›®æ¨¡å— ---
script_path = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(script_path))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# --- é¡¹ç›®æ¨¡å—å¯¼å…¥ ---
import pdfplumber
from data_processing.scraper import (
    extract_section_from_text,
    remove_tables,
    extract_qa_with_ai
)
# [V2] æ–°å¢å¯¼å…¥ï¼Œç”¨äºåœ¨æµ‹è¯•è„šæœ¬ä¸­æ¨¡æ‹Ÿåˆ†å—å¹¶ä¿å­˜ç»“æœ
from langchain_text_splitters import RecursiveCharacterTextSplitter
from config.settings import settings
# from scripts.to_chunks import embed_and_store_disclosure_chunks, embedder_instance # <-- DBæ“ä½œï¼Œç¦ç”¨
# from db.database import SessionLocal # <-- DBæ“ä½œï¼Œç¦ç”¨
# from db.models import StockDisclosure, StockDisclosureChunk # <-- DBæ“ä½œï¼Œç¦ç”¨

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("PipelineTest")

# --- æµ‹è¯•é…ç½® ---
# [V2] åˆ›å»ºä¸“é—¨çš„æµ‹è¯•è¾“å‡ºç›®å½•
TEST_OUTPUT_DIR = os.path.join(project_root, "tests", "output_test")
os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)
logger.info(f"Test outputs will be saved to: {TEST_OUTPUT_DIR}")

# æµ‹è¯•PDFæ–‡ä»¶ä½äº data_processing/reports/ ç›®å½•ä¸‹
PDF_BASE_PATH = os.path.join(project_root, "data_processing", "reports")
TEST_NARRATIVE_PDF_PATH = os.path.join(PDF_BASE_PATH, "test.pdf")
TEST_QA_PDF_PATH = os.path.join(PDF_BASE_PATH, "test2.pdf")

def read_pdf_text(file_path: str) -> str:
    """ä»æœ¬åœ°PDFæ–‡ä»¶è·¯å¾„è¯»å–æ‰€æœ‰æ–‡æœ¬ã€‚"""
    if not os.path.exists(file_path):
        logger.error(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return ""
    try:
        with open(file_path, 'rb') as f:
            with pdfplumber.open(f) as pdf:
                return "\n".join(page.extract_text() for page in pdf.pages if page.extract_text())
    except Exception as e:
        logger.error(f"è¯»å–æˆ–è§£æPDFæ—¶å‡ºé”™ {file_path}: {e}", exc_info=True)
        return ""


def run_data_processing_and_logging_test():
    """
    ã€V3 - çº¯æ—¥å¿—ç‰ˆã€‘
    æ‰§è¡Œæ•°æ®æå–å’Œåˆ†å—æµç¨‹ï¼Œä½†è·³è¿‡æ‰€æœ‰æ•°æ®åº“æ“ä½œï¼Œä»…å°†ç»“æœä¿å­˜ä¸ºæ–‡ä»¶ã€‚
    """
    logger.info("=" * 80)
    logger.info("ğŸš€ å¼€å§‹æ•°æ®å¤„ç†ä¸æ—¥å¿—è®°å½•æµ‹è¯• (æ•°æ®åº“æ“ä½œå·²ç¦ç”¨) ğŸš€")
    logger.info("=" * 80)

    try:
        # --- 1. å¤„ç†å¹´æŠ¥/åŠå¹´æŠ¥ (test.pdf) ---
        logger.info("\n---ã€ä»»åŠ¡1: å¤„ç†å¹´æŠ¥/åŠå¹´æŠ¥ã€‘---")
        logger.info(f"è¯»å–æ–‡ä»¶: {TEST_NARRATIVE_PDF_PATH}")
        narrative_full_text = read_pdf_text(TEST_NARRATIVE_PDF_PATH)
        if narrative_full_text:
            logger.info("æ­¥éª¤ 1/2: [Scraper] æå–'ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ'ç« èŠ‚å¹¶æ¸…ç†...")
            narrative_section = extract_section_from_text(narrative_full_text, "ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ")
            raw_content_narrative = remove_tables(narrative_section) if narrative_section else ""

            if raw_content_narrative:
                output_path = os.path.join(TEST_OUTPUT_DIR, "1_narrative_cleaned_content.txt")
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(raw_content_narrative)
                logger.info(f"âœ… å·²ä¿å­˜æ¸…ç†åçš„å¹´æŠ¥å†…å®¹åˆ°: {output_path}")

                logger.info("æ­¥éª¤ 2/2: [Chunking] æ¨¡æ‹Ÿåˆ†å—...")
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=settings.CHUNK_SIZE,
                    chunk_overlap=settings.CHUNK_OVERLAP,
                    separators=settings.TEXT_SPLITTER_SEPARATORS
                )
                narrative_chunks = text_splitter.split_text(raw_content_narrative)
                chunk_output_path = os.path.join(TEST_OUTPUT_DIR, "2_narrative_chunks.txt")
                with open(chunk_output_path, "w", encoding="utf-8") as f:
                    f.write(f"--- Document: {os.path.basename(TEST_NARRATIVE_PDF_PATH)} ---\n")
                    f.write(f"--- Total Chunks: {len(narrative_chunks)} ---\n\n")
                    for i, chunk in enumerate(narrative_chunks):
                        f.write(f"--- CHUNK {i+1}/{len(narrative_chunks)} (Length: {len(chunk)}) ---\n")
                        f.write(chunk.strip())
                        f.write("\n\n")
                logger.info(f"âœ… å·²ä¿å­˜å¹´æŠ¥åˆ†å—ç»“æœåˆ°: {chunk_output_path}")
            else:
                logger.warning("å¹´æŠ¥æœªèƒ½æå–åˆ°æœ‰æ•ˆå†…å®¹ã€‚")

        # --- 2. å¤„ç†è°ƒç ”æ´»åŠ¨çºªè¦ (é‡æ–°å¯ç”¨) ---
        logger.info("\n---ã€ä»»åŠ¡2: å¤„ç†Q&Aè°ƒç ”çºªè¦ã€‘---")
        logger.info(f"è¯»å–æ–‡ä»¶: {TEST_QA_PDF_PATH}")
        try:
            qa_full_text = read_pdf_text(TEST_QA_PDF_PATH)
            if qa_full_text and qa_full_text.strip():
                logger.info("æ­¥éª¤ 1/2: [Scraper] ä½¿ç”¨AIæå–Q&A...")

                # --- V4 å…³é”®ä¿®æ”¹: ä½¿ç”¨å·¥å‚æ¨¡å¼,ä¸å†ç¡¬ç¼–ç æ¨¡å‹ ---
                # æ­¤è„šæœ¬ç°åœ¨å°†æ ¹æ® settings.py ä¸­çš„ LLM_PROVIDER è®¾ç½®è‡ªåŠ¨é€‰æ‹©æä¾›å•†ã€‚
                # æˆ‘ä»¬ä¸å†ä¼ é€’ model_overrideï¼Œè®©æ¯ä¸ªæä¾›å•†ä½¿ç”¨å…¶æœ€åˆé€‚çš„é»˜è®¤æ¨¡å‹ã€‚
                qa_list = extract_qa_with_ai(qa_full_text)

                if qa_list:
                    # å°†Q&Aåˆ—è¡¨æ ¼å¼åŒ–ä¸ºJSONå­—ç¬¦ä¸²è¿›è¡Œå­˜å‚¨
                    raw_content_qa = json.dumps(qa_list, ensure_ascii=False, indent=4)
                    output_path = os.path.join(TEST_OUTPUT_DIR, "3_qa_cleaned_content.json")
                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(raw_content_qa)
                    logger.info(f"âœ… å·²ä¿å­˜Q&Aæå–ç»“æœåˆ°: {output_path}")

                    logger.info("æ­¥éª¤ 2/2: [Chunking] æ¨¡æ‹Ÿåˆ†å—...")
                    # å¯¹äºQ&Aï¼Œæ¯ä¸ªé—®ç­”å¯¹å°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„chunk
                    qa_chunks = [f"é—®é¢˜ï¼š{item.get('question', '')}\nå›ç­”ï¼š{item.get('answer', '')}" for item in qa_list]
                    chunk_output_path = os.path.join(TEST_OUTPUT_DIR, "4_qa_chunks.txt")
                    with open(chunk_output_path, "w", encoding="utf-8") as f:
                        f.write(f"--- Document: {os.path.basename(TEST_QA_PDF_PATH)} ---\n")
                        f.write(f"--- Total Chunks: {len(qa_chunks)} ---\n\n")
                        for i, chunk in enumerate(qa_chunks):
                            f.write(f"--- CHUNK {i+1}/{len(qa_chunks)} (Length: {len(chunk)}) ---\n")
                            f.write(chunk.strip())
                            f.write("\n\n")
                    logger.info(f"âœ… å·²ä¿å­˜Q&Aåˆ†å—ç»“æœåˆ°: {chunk_output_path}")
                else:
                    logger.warning("AIæœªèƒ½ä»Q&Aæ–‡ä»¶ä¸­æå–ä»»ä½•å†…å®¹ã€‚")
            else:
                logger.warning("æœªèƒ½ä»Q&A PDFä¸­æå–ä»»ä½•æ–‡æœ¬ã€‚")
        except Exception as e:
            logger.error(f"å¤„ç†Q&Aæ–‡ä»¶æ—¶å‡ºé”™: {e}", exc_info=True)

    except Exception as e:
        logger.error(f"æµ‹è¯•æµæ°´çº¿å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)

    logger.info("\n" + "=" * 80)
    logger.info("âœ… æ•°æ®å¤„ç†ä¸æ—¥å¿—è®°å½•æµ‹è¯•æ‰§è¡Œå®Œæ¯•ã€‚")
    logger.info(f"ğŸ‘‰ è¯·æ£€æŸ¥è¾“å‡ºç›®å½•: {TEST_OUTPUT_DIR}")
    logger.info("=" * 80)


if __name__ == "__main__":
    run_data_processing_and_logging_test() 